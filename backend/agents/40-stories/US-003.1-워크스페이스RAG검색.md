# 워크스페이스 RAG 검색 시나리오 사양서 (Workspace RAG Search with Hybrid Retrieval)
**문서 버전:** 1.0
**작성일:** 2025-10-08 10:00:00 KST

---

## 1) 개요
Arcana 사용자가 자신의 워크스페이스 문서만을 기반으로 질문을 던졌을 때, RAG(Retrieval-Augmented Generation) 파이프라인이 하이브리드 검색(벡터 + BM25)과 선택적 Cohere 재정렬을 적용해 근거가 포함된 답변을 돌려주는 시나리오를 정의합니다. 본 시나리오는 `/aiagent/search` API를 통해 LLM이 안전하게 컨텍스트를 구성하고, 응답에 인용 정보를 포함하는 것을 목표로 합니다.

---

## 2) 선행 조건
- 사용자는 Arcana에 로그인해 JWT를 보유하고 있어야 하며, 토큰으로 식별 가능한 워크스페이스가 존재해야 합니다.
- 환경 변수
  - `CM_AZURE_OPENAI_API_KEY`, `CM_AZURE_OPENAI_ENDPOINT`, `CM_AZURE_OPENAI_API_VERSION`, `CM_AZURE_OPENAI_CHAT_DEPLOYMENT` (LLM 응답용)
  - `EM_AZURE_OPENAI_API_KEY`, `EM_AZURE_OPENAI_ENDPOINT`, `EM_AZURE_OPENAI_API_VERSION`, `EM_AZURE_OPENAI_EMBEDDING_DEPLOYMENT` (임베딩/벡터 검색용)
  - `COHERE_API_KEY` (선택: rerank 사용 시)
  - `COHERE_RERANK_MODEL` (선택: 기본값 `rerank-english-v3.0`, 한국어 대응 모델로 교체 가능)
- 워크스페이스별 RAG 인덱스가 `backend/storage/workspace/<workspace_name>` 혹은 별도 `storage_uri`에 준비돼 있어야 합니다.
- BM25 검색을 위한 원본 Document 메타데이터가 `ChromaRAGService`에 적재되어 있어야 합니다 (upsert 시 자동 관리).

---

## 3) 주요 컴포넌트
- `backend/routers/aiagent.py`: `/aiagent/search` 라우터. JWT에서 사용자와 워크스페이스를 확인하고 요청을 LangChain 에이전트에 위임합니다.
- `backend/utils/workspace.py`: 워크스페이스 컨텍스트(식별자, 이름, RAG 스토리지 경로)를 조회합니다.
- `backend/ai_module/rag_search.py`:
  - `WorkspaceRAGSearchAgent`: 검색 전략(Vector/Keyword/Hybrid) 선택, Cohere rerank 적용, LLM 프롬프트 실행을 담당합니다.
  - `SearchStrategy` Enum 및 `SearchResult` 데이터 구조로 API와의 계약을 정의합니다.
- `backend/rag/chroma.py`: Chroma 기반 벡터 스토어 + BM25 인덱스를 보관하고, Reciprocal Rank Fusion(RRF) 기반 하이브리드 후보군을 반환합니다.
- `backend/schema/aiagent.py`: 요청/응답 스키마(`SearchRequest`, `SearchResponse`)로 클라이언트와의 데이터 계약을 명시합니다.

---

## 4) 상세 흐름
1. 클라이언트는 JWT를 Authorization 헤더에 담아 `/aiagent/search` POST 요청을 전송합니다.
   - Body: `query`(필수), `top_k`, `strategy`(vector/keyword/hybrid), `rerank_provider`("cohere"), `rerank_top_n`, `hybrid_alpha` 등을 선택적으로 설정합니다.
2. `aiagent` 라우터가 `get_workspace_context`를 통해 사용자에게 허용된 워크스페이스 인덱스/이름과 RAG 저장 위치를 확인합니다.
3. `WorkspaceRAGSearchAgent.search`는 요청 파라미터를 검증하고 후보 개수(`candidate_k`)를 조정합니다.
4. 선택된 검색 전략에 따라 다음 중 하나가 수행됩니다.
   - **Vector**: Chroma의 벡터 리트리버를 통해 유사도 점수 순으로 후보를 수집합니다.
   - **Keyword(BM25)**: 동일한 문서 집합에 대해 BM25 랭킹 점수를 계산합니다.
   - **Hybrid**: 벡터와 BM25 결과를 각각 수집한 뒤, RRF(Reciprocal Rank Fusion)로 점수를 합산하고 `hybrid_alpha` 가중치로 정렬합니다.
5. `rerank_provider`가 Cohere로 지정된 경우, 수집된 후보 중 상위 `rerank_top_n`개를 Cohere Rerank API로 재정렬합니다.
6. 상위 `top_k` 문서를 선택해 컨텍스트 블록을 구성하고, `[번호]` 인용 표기와 함께 LLM 프롬프트를 실행해 답변을 생성합니다.
7. 응답 객체에는 질문, 답변 본문, 각 문서의 `page_id`, `page_title`, `page_url`, `chunk_id`, `context_index` 등이 포함되어 클라이언트가 근거를 표시할 수 있습니다.

---

## 5) 예외 처리 및 롤백
- 필수 환경 변수가 누락되면 500 오류를 반환하며 로그에 원인을 남깁니다.
- 지원하지 않는 검색 전략 또는 비어 있는 질의는 400 오류로 응답합니다.
- Cohere API 키나 패키지가 없는 경우 rerank 단계는 건너뛰고 경고 로그만 남깁니다.
- 벡터/BM25 후보가 비어 있을 경우 “관련 문서를 찾을 수 없습니다.”라는 기본 응답을 반환합니다.

---

## 6) 검증 포인트
- 동일 워크스페이스 내 문서만 검색되고 타 워크스페이스 데이터가 섞이지 않는지 확인합니다.
- Hybrid 전략 선택 시 벡터와 BM25 결과가 균형 있게 섞여 상위 랭킹이 개선되는지 확인합니다.
- Cohere rerank 활성화 후 응답이 `top_k` 범위 내에서 재정렬되는지, 그리고 인용 번호와 `context_index`가 일치하는지 검증합니다.
- LLM 응답이 컨텍스트 범위를 벗어난 정보를 참조하지 않는지 QA합니다.

---

## 7) 후속 과제
- LangGraph 기반 멀티 노드 워크플로우로 “검색 전용 ↔ 생성(파일 작성)” 플로우를 분기하는 기능 검토.
- 한국어에 특화된 형태소 분석기를 접목해 BM25 품질을 향상하는 방안 조사.
- 검색 히스토리/피드백을 저장해 랭킹 가중치나 rerank 모델을 동적으로 조정하는 기능 추가.
